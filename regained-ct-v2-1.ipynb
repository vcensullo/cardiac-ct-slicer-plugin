{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":2343,"sourceType":"datasetVersion","datasetId":1012},{"sourceId":2764486,"sourceType":"datasetVersion","datasetId":1686903},{"sourceId":3610416,"sourceType":"datasetVersion","datasetId":2126553},{"sourceId":6685411,"sourceType":"datasetVersion","datasetId":3856259}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# CELLA 0: PULIZIA TOTALE E RESET AMBIENTE\nprint(\"üßπ PULIZIA TOTALE AMBIENTE E RESET WORKSPACE\")\nprint(\"=\"*70)\n\nimport os\nimport shutil\nimport glob\nfrom pathlib import Path\nimport gc\nimport torch\n\n# Torna alla directory root\nos.chdir('/kaggle/working')\n\nprint(\"üìç Directory corrente:\", os.getcwd())\nprint(\"\\nüîç ANALISI PRE-PULIZIA:\")\n\n# Funzione per calcolare dimensione directory\ndef get_dir_size(path):\n    total = 0\n    try:\n        for entry in os.scandir(path):\n            if entry.is_file():\n                total += entry.stat().st_size\n            elif entry.is_dir():\n                total += get_dir_size(entry.path)\n    except:\n        pass\n    return total\n\n# Mostra stato attuale\ntotal_size = 0\nitems_to_remove = []\n\nfor item in os.listdir('.'):\n    if item.startswith('.'):  # Skip hidden files\n        continue\n    \n    item_path = os.path.join('.', item)\n    \n    if os.path.isdir(item_path):\n        size = get_dir_size(item_path) / (1024**2)  # MB\n        total_size += size\n        print(f\"   üìÅ {item}/ - {size:.1f} MB\")\n        \n        # Aggiungi alla lista di rimozione (tranne dataset)\n        if not item.startswith('kaggle') and item != 'input':\n            items_to_remove.append(item_path)\n    else:\n        size = os.path.getsize(item_path) / (1024**2)  # MB\n        total_size += size\n        if size > 0.1:  # Mostra solo file > 0.1 MB\n            print(f\"   üìÑ {item} - {size:.1f} MB\")\n        \n        # Aggiungi alla lista di rimozione\n        if not item.endswith('.ipynb'):  # Mantieni notebook\n            items_to_remove.append(item_path)\n\nprint(f\"\\nüíæ Spazio totale utilizzato: {total_size:.1f} MB\")\nprint(f\"üóëÔ∏è  Elementi da rimuovere: {len(items_to_remove)}\")\n\n# PULIZIA AGGRESSIVA\nprint(\"\\nüî• INIZIO PULIZIA AGGRESSIVA...\")\n\n# Pattern di directory da rimuovere\ndirectories_to_remove = [\n    'ReGAINED-CT*',\n    'MedSRGAN*',\n    'checkpoints*',\n    'samples*',\n    'results*',\n    'evaluation*',\n    'dicom_results*',\n    'logs*',\n    'debug*',\n    'models*',\n    'data*',\n    '__pycache__*',\n    '.ipynb_checkpoints*',\n    'training_outputs*',\n    'test_*',\n    'output*',\n    'temp*',\n    'cache*'\n]\n\n# Pattern di file da rimuovere\nfiles_to_remove = [\n    '*.pth',\n    '*.pt',\n    '*.pkl',\n    '*.png',\n    '*.jpg',\n    '*.jpeg',\n    '*.json',\n    '*.log',\n    '*.txt',\n    '*.h5',\n    '*.hdf5',\n    '*.npy',\n    '*.npz',\n    '*.csv',\n    '*.pdf'\n]\n\nremoved_count = 0\nfreed_space = 0\n\n# Rimuovi directory\nfor pattern in directories_to_remove:\n    for path in glob.glob(pattern):\n        if os.path.isdir(path):\n            try:\n                size = get_dir_size(path) / (1024**2)\n                shutil.rmtree(path)\n                print(f\"   üóëÔ∏è  Rimossa directory: {path} ({size:.1f} MB)\")\n                removed_count += 1\n                freed_space += size\n            except Exception as e:\n                print(f\"   ‚ö†Ô∏è  Errore rimozione {path}: {e}\")\n\n# Rimuovi file\nfor pattern in files_to_remove:\n    for path in glob.glob(pattern):\n        if os.path.isfile(path):\n            try:\n                size = os.path.getsize(path) / (1024**2)\n                os.remove(path)\n                if size > 0.1:  # Mostra solo file significativi\n                    print(f\"   üóëÔ∏è  Rimosso file: {path} ({size:.1f} MB)\")\n                removed_count += 1\n                freed_space += size\n            except Exception as e:\n                print(f\"   ‚ö†Ô∏è  Errore rimozione {path}: {e}\")\n\n# Pulizia memoria Python\ngc.collect()\n\n# Pulizia CUDA se disponibile\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n    torch.cuda.synchronize()\n    print(\"\\nüñ•Ô∏è  Cache CUDA pulita\")\n\n# Verifica spazio su disco\ndisk_usage = shutil.disk_usage('.')\nprint(f\"\\nüíæ STATO DISCO DOPO PULIZIA:\")\nprint(f\"   Totale: {disk_usage.total/1024**3:.1f} GB\")\nprint(f\"   Usato: {disk_usage.used/1024**3:.1f} GB\")\nprint(f\"   Libero: {disk_usage.free/1024**3:.1f} GB\")\n\nprint(f\"\\n‚úÖ PULIZIA COMPLETATA:\")\nprint(f\"   Elementi rimossi: {removed_count}\")\nprint(f\"   Spazio liberato: {freed_space:.1f} MB\")\n\n# Mostra stato finale\nprint(f\"\\nüìä STATO FINALE:\")\nremaining_items = []\nfor item in os.listdir('.'):\n    if not item.startswith('.') and item != 'input':\n        remaining_items.append(item)\n\nprint(f\"   Elementi rimanenti: {len(remaining_items)}\")\nfor item in sorted(remaining_items):\n    print(f\"      ‚Ä¢ {item}\")\n\nprint(f\"\\nüéØ AMBIENTE PULITO E PRONTO!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T09:07:59.763468Z","iopub.execute_input":"2025-07-07T09:07:59.763783Z","iopub.status.idle":"2025-07-07T09:08:05.495862Z","shell.execute_reply.started":"2025-07-07T09:07:59.763761Z","shell.execute_reply":"2025-07-07T09:08:05.494803Z"},"collapsed":true,"jupyter":{"source_hidden":true,"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"üßπ PULIZIA TOTALE AMBIENTE E RESET WORKSPACE\n======================================================================\nüìç Directory corrente: /kaggle/working\n\nüîç ANALISI PRE-PULIZIA:\n   üìÅ data/ - 0.0 MB\n   üìÅ debug/ - 11.8 MB\n   üìÅ MedSRGAN/ - 0.7 MB\n   üìÅ ReGAINED-CT/ - 253.8 MB\n\nüíæ Spazio totale utilizzato: 266.3 MB\nüóëÔ∏è  Elementi da rimuovere: 5\n\nüî• INIZIO PULIZIA AGGRESSIVA...\n   üóëÔ∏è  Rimossa directory: ReGAINED-CT (253.8 MB)\n   üóëÔ∏è  Rimossa directory: MedSRGAN (0.7 MB)\n   üóëÔ∏è  Rimossa directory: debug (11.8 MB)\n   üóëÔ∏è  Rimossa directory: data (0.0 MB)\n\nüíæ STATO DISCO DOPO PULIZIA:\n   Totale: 19.5 GB\n   Usato: 0.0 GB\n   Libero: 19.5 GB\n\n‚úÖ PULIZIA COMPLETATA:\n   Elementi rimossi: 4\n   Spazio liberato: 266.3 MB\n\nüìä STATO FINALE:\n   Elementi rimanenti: 1\n      ‚Ä¢ state.db\n\nüèóÔ∏è  CREAZIONE STRUTTURA PULITA...\n   üìÅ checkpoints/ creata\n   üìÅ samples/ creata\n   üìÅ results/ creata\n   üìÅ evaluation/ creata\n   üìÅ dicom_results/ creata\n   üìÅ logs/ creata\n   üìÅ data/ creata\n   üìÅ debug/ creata\n\nüéØ AMBIENTE PULITO E PRONTO!\nüìç Directory di lavoro: /kaggle/working/ReGAINED-CT\nüöÄ Pronto per iniziare training pulito con MedSRGAN\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# CELLA 1: Setup progetto ReGAINED-CT\nprint(\"üè• ReGAINED-CT - SETUP PROGETTO\")\nprint(\"=\"*60)\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport torch\nimport torch.nn as nn\nfrom collections import defaultdict\nimport shutil\nimport json\n\n# Configurazione ReGAINED-CT\nCONFIG = {\n    'project': {\n        'name': 'ReGAINED-CT',\n        'full_name': 'Reconstruction Enhanced GANs for CT Denoising and Super-Resolution',\n        'version': '1.0',\n        'approach': 'Combined Denoising + 4x Upscaling'\n    },\n    'datasets': {\n        'primary': 'CT Low Dose (denoising + upscaling)',\n        'input_size': 96,\n        'output_size': 384,\n        'scale_factor': 4,\n        'normalization': '[0, 1]'\n    },\n    'model': {\n        'generator': 'MedSRGAN Generator (128 RWMAB blocks)',\n        'discriminator': 'Custom Discriminator for 384x384',\n        'channels': 3\n    }\n}\n\nprint(f\"üìã PROGETTO: {CONFIG['project']['name']}\")\nprint(f\"üìñ Nome completo: {CONFIG['project']['full_name']}\")\nprint(f\"üéØ Approccio: {CONFIG['project']['approach']}\")\nprint(f\"üìä Dataset: {CONFIG['datasets']['primary']}\")\nprint(f\"üìê Input: {CONFIG['datasets']['input_size']}x{CONFIG['datasets']['input_size']}\")\nprint(f\"üìê Output: {CONFIG['datasets']['output_size']}x{CONFIG['datasets']['output_size']}\")\n\n# Verifica hardware\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"\\nüñ•Ô∏è  Hardware:\")\nprint(f\"   Device: {device}\")\nif torch.cuda.is_available():\n    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n    print(f\"   CUDA Version: {torch.version.cuda}\")\n\n# Crea directory progetto\nbase_dir = \"ReGAINED-CT\"\nos.makedirs(base_dir, exist_ok=True)\nos.chdir(base_dir)\nprint(f\"\\nüìÅ Directory di lavoro: {os.getcwd()}\")\n\n# Crea sottodirectory\nsubdirs = [\"models\", \"data\", \"results\", \"logs\", \"checkpoints\", \"debug\", \"samples\", \"evaluation\", \"dicom_results\"]\nfor subdir in subdirs:\n    os.makedirs(subdir, exist_ok=True)\n    print(f\"   üìÅ {subdir}/ creata\")\n\n# Salva configurazione\nwith open('config.json', 'w') as f:\n    json.dump(CONFIG, f, indent=2)\nprint(f\"\\nüíæ Configurazione salvata in config.json\")\n\nprint(f\"\\n‚úÖ Setup progetto completato!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T09:09:20.152392Z","iopub.execute_input":"2025-07-07T09:09:20.153222Z","iopub.status.idle":"2025-07-07T09:09:20.762546Z","shell.execute_reply.started":"2025-07-07T09:09:20.153185Z","shell.execute_reply":"2025-07-07T09:09:20.761527Z"},"jupyter":{"outputs_hidden":true,"source_hidden":true},"collapsed":true},"outputs":[{"name":"stdout","text":"üè• ReGAINED-CT - SETUP PROGETTO E CLONE MEDSRGAN\n============================================================\nüìã PROGETTO: ReGAINED-CT\nüìñ Nome completo: Reconstruction of Enhanced GANs for Image-based Noise Elimination and Denoising in CT\nüéØ Target: 3D Slicer Module\nüìä Strategia dataset:\n   ü•á Primario: CT Low Dose (denoising)\n   ü•à Secondario: CT Kidney (variety)\n   ü•â Terziario: CT Brain (pre-training)\n   ‚úÖ Validazione: SIIM Medical (testing)\n\nüñ•Ô∏è  Hardware:\n   Device: cpu\n   GPU Available: ‚ùå\n\nüìÅ Directory progetto: /kaggle/working/ReGAINED-CT/ReGAINED-CT/ReGAINED-CT\n\nüîÑ CLONING MEDSRGAN DA GITHUB:\nCloning into 'MedSRGAN'...\nremote: Enumerating objects: 44, done.\u001b[K\nremote: Counting objects: 100% (44/44), done.\u001b[K\nremote: Compressing objects: 100% (42/42), done.\u001b[K\nremote: Total 44 (delta 18), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (44/44), 358.95 KiB | 14.36 MiB/s, done.\nResolving deltas: 100% (18/18), done.\n‚úÖ MedSRGAN clonato!\n\nüîç DEBUG - STRUTTURA MEDSRGAN:\n   üìÅ MedSRGAN/\n      üìÅ .git/\n      üìÑ README.md (422 bytes)\n      üìÑ dataset.py (1006 bytes)\n      üìÑ discriminator.py (2121 bytes)\n      üìÑ generator.py (2046 bytes)\n      üìÅ img/\n      üìÑ trainer.py (4084 bytes)\n   üìÅ models/ creata\n   üìÅ data/ creata\n   üìÅ results/ creata\n   üìÅ logs/ creata\n   üìÅ checkpoints/ creata\n   üìÅ debug/ creata\n   üìÅ slicer_module/ creata\n   üìÑ config.json salvato\n\n‚úÖ Setup ReGAINED-CT completato con GPU attivata!\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# CELLA 2: Clone MedSRGAN da GitHub\nprint(\"üîÑ CLONE MEDSRGAN\")\nprint(\"=\"*60)\n\nimport os\n\n# Clone MedSRGAN se non esiste\nif not os.path.exists(\"MedSRGAN\"):\n    print(\"üì• Clonando MedSRGAN da GitHub...\")\n    !git clone https://github.com/04RR/MedSRGAN.git\n    print(\"‚úÖ MedSRGAN clonato con successo!\")\nelse:\n    print(\"‚úÖ MedSRGAN gi√† presente\")\n\n# Verifica struttura\nprint(f\"\\nüìÅ Verifica struttura MedSRGAN:\")\nif os.path.exists(\"MedSRGAN\"):\n    important_files = ['generator.py', 'discriminator.py', 'train.py', 'test.py']\n    \n    for file in important_files:\n        file_path = os.path.join(\"MedSRGAN\", file)\n        if os.path.exists(file_path):\n            size = os.path.getsize(file_path) / 1024  # KB\n            print(f\"   ‚úÖ {file} ({size:.1f} KB)\")\n        else:\n            print(f\"   ‚ùå {file} non trovato\")\n    \n    # Mostra altri file\n    print(f\"\\nüìÑ Altri file in MedSRGAN:\")\n    all_files = os.listdir(\"MedSRGAN\")\n    other_files = [f for f in all_files if f not in important_files and not f.startswith('.')]\n    for file in sorted(other_files)[:5]:  # Mostra primi 5\n        print(f\"   ‚Ä¢ {file}\")\n    \n    if len(other_files) > 5:\n        print(f\"   ... e altri {len(other_files) - 5} file\")\n\nprint(f\"\\n‚úÖ MedSRGAN pronto per l'uso!\")\nprint(f\"‚ÑπÔ∏è  Useremo l'architettura originale con:\")\nprint(f\"   ‚Ä¢ Generator: 8 blocks √ó 16 RWMAB = 128 RWMAB totali\")\nprint(f\"   ‚Ä¢ Discriminator: Custom per 384x384\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T09:09:26.640498Z","iopub.execute_input":"2025-07-07T09:09:26.640843Z","iopub.status.idle":"2025-07-07T09:09:26.650139Z","shell.execute_reply.started":"2025-07-07T09:09:26.640812Z","shell.execute_reply":"2025-07-07T09:09:26.648896Z"},"jupyter":{"source_hidden":true}},"outputs":[{"name":"stdout","text":"üè• ReGAINED-CT - SETUP PROGETTO\n============================================================\n\nüîÑ CLONING MEDSRGAN DA GITHUB:\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# CELLA 3 CORRETTA: Esplorazione e preparazione dataset ReGAINED-CT (FIX)\nprint(\"üìä ESPLORAZIONE E PREPARAZIONE DATASET\")\nprint(\"=\"*50)\n\n# Import necessari\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport json\nfrom collections import defaultdict\n\n# Verifica e crea directory necessarie\nrequired_dirs = ['debug', 'data', 'results', 'logs']\nfor dir_name in required_dirs:\n    if not os.path.exists(dir_name):\n        os.makedirs(dir_name)\n        print(f\"   üìÅ {dir_name}/ creata\")\n\n# Percorsi dataset\ndataset_paths = {\n    'ct_brain': '/kaggle/input/computed-tomography-ct-of-the-brain/',\n    'ct_kidney': '/kaggle/input/ct-kidney-dataset-normal-cyst-tumor-and-stone/',\n    'ct_low_dose': '/kaggle/input/ct-low-dose-reconstruction/',\n    'siim_medical': '/kaggle/input/siim-medical-images/'\n}\n\n# Riusa i risultati precedenti (visto che abbiamo gi√† i dati)\ndataset_info = {\n    'ct_brain': {\n        'total_files': 519,\n        'file_counts': {'.csv': 1, '.dcm': 259, '.jpg': 259},\n        'sample_files': {},\n        'path': dataset_paths['ct_brain']\n    },\n    'ct_kidney': {\n        'total_files': 12447,\n        'file_counts': {'.csv': 1, '.jpg': 12446},\n        'sample_files': {},\n        'path': dataset_paths['ct_kidney']\n    },\n    'ct_low_dose': {\n        'total_files': 133025,\n        'file_counts': {'.csv': 1, '.ima': 33256, '.png': 99768},\n        'sample_files': {},\n        'path': dataset_paths['ct_low_dose']\n    },\n    'siim_medical': {\n        'total_files': 202,\n        'file_counts': {'.csv': 1, '.dcm': 100, '.npz': 1, '.tif': 100},\n        'sample_files': {},\n        'path': dataset_paths['siim_medical']\n    }\n}\n\nprint(\"üîç DATI DATASET GI√Ä ANALIZZATI:\")\nfor name, info in dataset_info.items():\n    print(f\"\\nüè• {name.upper()}:\")\n    print(f\"   üìä Totale file: {info['total_files']:,}\")\n    print(f\"   üìÅ Tipi file principali:\")\n    for ext, count in info['file_counts'].items():\n        if ext in ['.png', '.jpg', '.dcm', '.tif']:\n            print(f\"      {ext}: {count:,}\")\n\n# Trova file di esempio per visualizzazione\ndef find_sample_files(dataset_name, path, target_exts):\n    samples = {}\n    if not os.path.exists(path):\n        return samples\n    \n    for root, dirs, files in os.walk(path):\n        for file in files:\n            ext = os.path.splitext(file)[1].lower()\n            if ext in target_exts and ext not in samples:\n                samples[ext] = os.path.join(root, file)\n                break\n        if len(samples) >= len(target_exts):\n            break\n    return samples\n\n# Trova campioni per visualizzazione\nprint(f\"\\nüîç RICERCA CAMPIONI PER VISUALIZZAZIONE:\")\nfor name, info in dataset_info.items():\n    target_exts = [ext for ext in info['file_counts'].keys() if ext in ['.png', '.jpg', '.dcm', '.tif']]\n    samples = find_sample_files(name, info['path'], target_exts)\n    info['sample_files'] = samples\n    print(f\"   {name}: {len(samples)} campioni trovati\")\n\n# üîç DEBUG: Visualizzazione campioni\nprint(f\"\\nüîç DEBUG - VISUALIZZAZIONE CAMPIONI:\")\n\nfig, axes = plt.subplots(2, 4, figsize=(20, 10))\naxes = axes.flatten()\n\nsample_idx = 0\nfor dataset_name, info in dataset_info.items():\n    if info['sample_files']:\n        for ext, sample_path in info['sample_files'].items():\n            if sample_idx < 8:\n                try:\n                    if ext == '.dcm':\n                        try:\n                            import pydicom\n                            ds = pydicom.dcmread(sample_path)\n                            img_array = ds.pixel_array\n                            title = f\"{dataset_name}\\n{ext} - {img_array.shape}\"\n                        except ImportError:\n                            # Skip DICOM se pydicom non disponibile\n                            continue\n                    elif ext == '.tif':\n                        img = Image.open(sample_path)\n                        img_array = np.array(img)\n                        title = f\"{dataset_name}\\n{ext} - {img.size}\"\n                    else:\n                        img = Image.open(sample_path)\n                        img_array = np.array(img)\n                        title = f\"{dataset_name}\\n{ext} - {img.size} - {img.mode}\"\n                    \n                    axes[sample_idx].imshow(img_array, cmap='gray')\n                    axes[sample_idx].set_title(title, fontsize=10)\n                    axes[sample_idx].axis('off')\n                    sample_idx += 1\n                    \n                except Exception as e:\n                    axes[sample_idx].text(0.5, 0.5, f'Errore\\n{str(e)[:30]}...', \n                                         ha='center', va='center', transform=axes[sample_idx].transAxes)\n                    axes[sample_idx].set_title(f\"{dataset_name}\\n{ext} - ERRORE\", fontsize=10)\n                    axes[sample_idx].axis('off')\n                    sample_idx += 1\n\n# Nascondi assi non usati\nfor i in range(sample_idx, 8):\n    axes[i].axis('off')\n\nplt.tight_layout()\n\n# Salvataggio sicuro\ntry:\n    plt.savefig('debug/dataset_samples.png', dpi=150, bbox_inches='tight')\n    print(\"   ‚úÖ Immagine salvata: debug/dataset_samples.png\")\nexcept Exception as e:\n    print(f\"   ‚ö†Ô∏è  Salvataggio immagine fallito: {e}\")\n\nplt.show()\n\n# Focus su CT Low Dose (dataset primario)\nprint(f\"\\nüéØ FOCUS SU CT LOW DOSE (DATASET PRIMARIO):\")\nct_path = dataset_info['ct_low_dose']['path']\n\n# Analizza metadata\nmetadata_path = os.path.join(ct_path, 'metadata.csv')\nif os.path.exists(metadata_path):\n    df = pd.read_csv(metadata_path)\n    print(f\"   üìã Metadata: {df.shape[0]:,} righe, {df.shape[1]} colonne\")\n    print(f\"   üë• Pazienti: {df['Id'].nunique()}\")\n    print(f\"   üìè Spessori: {df['Slice Thickness'].unique()}\")\n    print(f\"   üîß Kernel: {df['Reconstruction Kernel'].unique()}\")\n    \n    # üîç DEBUG: Prime righe metadata\n    print(f\"\\nüîç DEBUG - PRIME RIGHE METADATA:\")\n    print(df.head())\n    \n    # üîç DEBUG: Distribuzione per paziente\n    print(f\"\\nüîç DEBUG - DISTRIBUZIONE PER PAZIENTE:\")\n    patient_counts = df['Id'].value_counts()\n    print(patient_counts)\n    \nelse:\n    print(f\"   ‚ùå Metadata non trovato\")\n\n# Riepilogo per ReGAINED-CT\nprint(f\"\\nüìà RIEPILOGO PER ReGAINED-CT:\")\nprint(f\"   ü•á Primario (CT Low Dose): {dataset_info['ct_low_dose']['file_counts'].get('.png', 0):,} PNG\")\nprint(f\"   ü•à Secondario (CT Kidney): {dataset_info['ct_kidney']['file_counts'].get('.jpg', 0):,} JPG\")\nprint(f\"   ü•â Terziario (CT Brain): {dataset_info['ct_brain']['file_counts'].get('.dcm', 0):,} DICOM\")\nprint(f\"   ‚úÖ Validazione (SIIM): {dataset_info['siim_medical']['file_counts'].get('.dcm', 0):,} DICOM\")\n\n# Strategia di utilizzo\nprint(f\"\\nüéØ STRATEGIA DI UTILIZZO:\")\nprint(f\"   1. ü•á CT Low Dose: Training principale (denoising)\")\nprint(f\"      ‚Ä¢ 99,768 PNG grayscale\")\nprint(f\"      ‚Ä¢ Coppie Quarter/Full Dose\")\nprint(f\"      ‚Ä¢ 10 pazienti, multiple configurazioni\")\nprint(f\"   2. ü•à CT Kidney: Data augmentation\")\nprint(f\"      ‚Ä¢ 12,446 JPG ‚Üí convertire a grayscale\")\nprint(f\"      ‚Ä¢ 4 categorie: Normal, Cyst, Tumor, Stone\")\nprint(f\"   3. ü•â CT Brain: Pre-training/Fine-tuning\")\nprint(f\"      ‚Ä¢ 259 DICOM + 259 JPG\")\nprint(f\"      ‚Ä¢ 3 categorie: Aneurysm, Cancer, Tumor\")\nprint(f\"   4. ‚úÖ SIIM Medical: Testing finale\")\nprint(f\"      ‚Ä¢ 100 DICOM per validazione\")\n\n# Salva info dataset\ntry:\n    info_for_json = {}\n    for name, info in dataset_info.items():\n        info_copy = info.copy()\n        info_copy['sample_files'] = {k: str(v) for k, v in info['sample_files'].items()}\n        info_for_json[name] = info_copy\n    \n    with open('data/dataset_info.json', 'w') as f:\n        json.dump(info_for_json, f, indent=2)\n    \n    print(f\"\\nüíæ Info dataset salvate in data/dataset_info.json\")\n    \nexcept Exception as e:\n    print(f\"\\n‚ùå Errore salvataggio JSON: {e}\")\n\nprint(f\"\\n‚úÖ Esplorazione dataset completata!\")\nprint(f\"üéØ PRONTO PER LA CELLA 4: Preparazione CT Low Dose\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T08:57:20.819575Z","iopub.execute_input":"2025-07-07T08:57:20.820283Z","iopub.status.idle":"2025-07-07T08:57:30.694562Z","shell.execute_reply.started":"2025-07-07T08:57:20.820257Z","shell.execute_reply":"2025-07-07T08:57:30.693898Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELLA 4 CORRETTA: Preparazione dataset per approccio combinato\nprint(\"üéØ PREPARAZIONE DATASET CT - APPROCCIO COMBINATO\")\nprint(\"=\"*50)\n\nimport os\nimport glob\nfrom PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport json  # AGGIUNTO IMPORT MANCANTE\n\n# Carica le coppie CT\ndef load_ct_pairs_combined():\n    \"\"\"Carica coppie per denoising + upscaling combinato\"\"\"\n    dataset_path = \"/kaggle/input/ct-low-dose-reconstruction/\"\n    base_path = \"Preprocessed_384x384/384/\"\n    \n    quarter_path = os.path.join(dataset_path, base_path, \"Quarter Dose/1mm/Sharp Kernel (D45)/\")\n    full_path = os.path.join(dataset_path, base_path, \"Full Dose/1mm/Sharp Kernel (D45)/\")\n    \n    # Trova i file\n    quarter_files = glob.glob(os.path.join(quarter_path, \"**/*.png\"), recursive=True)\n    full_files = glob.glob(os.path.join(full_path, \"**/*.png\"), recursive=True)\n    \n    print(f\"üìä Files trovati: {len(quarter_files)} quarter, {len(full_files)} full\")\n    \n    # Estrai ID e crea mapping (codice esistente)\n    def extract_file_id(filepath):\n        filename = os.path.basename(filepath)\n        parts = filename.split('.')\n        if len(parts) >= 4:\n            patient_part = parts[0]\n            slice_part = parts[3]\n            patient_id = patient_part.split('_')[0]\n            return f\"{patient_id}_{slice_part}\"\n        return None\n    \n    # Crea coppie\n    quarter_mapping = {extract_file_id(f): f for f in quarter_files if extract_file_id(f)}\n    full_mapping = {extract_file_id(f): f for f in full_files if extract_file_id(f)}\n    \n    matched_ids = set(quarter_mapping.keys()) & set(full_mapping.keys())\n    ct_pairs = [(quarter_mapping[id], full_mapping[id]) for id in matched_ids]\n    \n    print(f\"üéØ Coppie trovate: {len(ct_pairs)}\")\n    return ct_pairs\n\n# Carica coppie\nct_pairs = load_ct_pairs_combined()\n\n# Visualizza esempi con approccio combinato\nprint(f\"\\nüîç VISUALIZZAZIONE APPROCCIO COMBINATO:\")\n\nfig, axes = plt.subplots(3, 3, figsize=(15, 12))\nfig.suptitle('ReGAINED-CT: Approccio Combinato (96x96 ‚Üí 384x384)', fontsize=16)\n\nfor i in range(3):\n    if i < len(ct_pairs):\n        quarter_path, full_path = ct_pairs[i]\n        \n        # Carica immagini originali\n        quarter_img = Image.open(quarter_path).convert('L')\n        full_img = Image.open(full_path).convert('L')\n        \n        # IMPORTANTE: Downscale quarter dose a 96x96 per input\n        quarter_small = quarter_img.resize((96, 96), Image.BICUBIC)\n        \n        # Visualizza\n        axes[i, 0].imshow(quarter_small, cmap='gray')\n        axes[i, 0].set_title(f'Input: Quarter Dose 96x96\\n(Downscaled + Noisy)')\n        axes[i, 0].axis('off')\n        \n        axes[i, 1].imshow(quarter_img, cmap='gray')\n        axes[i, 1].set_title(f'Original Quarter 384x384\\n(Solo per confronto)')\n        axes[i, 1].axis('off')\n        \n        axes[i, 2].imshow(full_img, cmap='gray')\n        axes[i, 2].set_title(f'Target: Full Dose 384x384\\n(Clean + High-Res)')\n        axes[i, 2].axis('off')\n\nplt.tight_layout()\nplt.savefig('debug/combined_approach_samples.png', dpi=150, bbox_inches='tight')\nplt.show()\n\n# Configurazione training combinato\ntraining_config = {\n    'dataset': {\n        'name': 'CT Low Dose - Combined Approach',\n        'total_pairs': len(ct_pairs),\n        'input_resolution': (96, 96),      # Quarter dose downscaled\n        'output_resolution': (384, 384),   # Full dose original\n        'scale_factor': 4,                 # 96 ‚Üí 384\n        'channels': 3,                     # RGB per MedSRGAN\n        'task': 'denoising + 4x upscaling'\n    },\n    'approach': {\n        'type': 'combined',\n        'denoising': True,\n        'super_resolution': True,\n        'benefits': [\n            'Rimozione rumore da quarter dose',\n            'Upscaling 4x simultaneo', \n            'Nessuna modifica architettura MedSRGAN',\n            'Massima stabilit√†'\n        ]\n    }\n}\n\nprint(f\"\\nüìã CONFIGURAZIONE APPROCCIO COMBINATO:\")\nprint(f\"   üéØ Task: {training_config['dataset']['task']}\")\nprint(f\"   üì• Input: {training_config['dataset']['input_resolution']} (noisy + low-res)\")\nprint(f\"   üì§ Output: {training_config['dataset']['output_resolution']} (clean + high-res)\")\nprint(f\"   üîç Scale factor: {training_config['dataset']['scale_factor']}x\")\nprint(f\"   üé® Canali: {training_config['dataset']['channels']} (RGB)\")\nprint(f\"   ‚úÖ Benefici:\")\nfor benefit in training_config['approach']['benefits']:\n    print(f\"      ‚Ä¢ {benefit}\")\n\n# Salva configurazione\nwith open('data/combined_training_config.json', 'w') as f:\n    json.dump(training_config, f, indent=2)\n\nprint(f\"\\n‚úÖ Setup approccio combinato completato!\")\nprint(f\"üöÄ PRONTO per usare MedSRGAN originale senza modifiche!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T08:57:38.031183Z","iopub.execute_input":"2025-07-07T08:57:38.031672Z","iopub.status.idle":"2025-07-07T08:57:53.548134Z","shell.execute_reply.started":"2025-07-07T08:57:38.031651Z","shell.execute_reply":"2025-07-07T08:57:53.547369Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELLA 5 CORRETTA: Dataset Loader con normalizzazione corretta\nprint(\"üìä CREAZIONE DATASET LOADER - NORMALIZZAZIONE CORRETTA\")\nprint(\"=\"*60)\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nimport numpy as np\n\nclass CTCombinedDataset(Dataset):\n    \"\"\"Dataset per approccio combinato - normalizzazione [0, 1] per Sigmoid\"\"\"\n    \n    def __init__(self, ct_pairs, input_size=96, output_size=384, augment=True):\n        self.ct_pairs = ct_pairs\n        self.input_size = input_size\n        self.output_size = output_size\n        self.augment = augment\n        \n        print(f\"üìä Dataset inizializzato:\")\n        print(f\"   Coppie: {len(self.ct_pairs)}\")\n        print(f\"   Normalizzazione: [0, 1] (per Sigmoid generator)\")\n    \n    def __len__(self):\n        return len(self.ct_pairs)\n    \n    def __getitem__(self, idx):\n        quarter_path, full_path = self.ct_pairs[idx]\n        \n        try:\n            # Carica immagini\n            quarter_img = Image.open(quarter_path).convert('L')\n            full_img = Image.open(full_path).convert('L')\n            \n            # Resize\n            quarter_small = quarter_img.resize((self.input_size, self.input_size), Image.BICUBIC)\n            full_img = full_img.resize((self.output_size, self.output_size), Image.BICUBIC)\n            \n            # Augmentation\n            if self.augment and np.random.random() > 0.5:\n                if np.random.random() > 0.5:\n                    quarter_small = quarter_small.transpose(Image.FLIP_LEFT_RIGHT)\n                    full_img = full_img.transpose(Image.FLIP_LEFT_RIGHT)\n                \n                if np.random.random() > 0.5:\n                    quarter_small = quarter_small.transpose(Image.FLIP_TOP_BOTTOM)\n                    full_img = full_img.transpose(Image.FLIP_TOP_BOTTOM)\n            \n            # Converti in RGB\n            quarter_rgb = quarter_small.convert('RGB')\n            full_rgb = full_img.convert('RGB')\n            \n            # Converti in array\n            quarter_array = np.array(quarter_rgb, dtype=np.float32)\n            full_array = np.array(full_rgb, dtype=np.float32)\n            \n            # NORMALIZZA A [0, 1] per Sigmoid!\n            quarter_norm = quarter_array / 255.0\n            full_norm = full_array / 255.0\n            \n            # To tensor\n            quarter_tensor = torch.FloatTensor(quarter_norm).permute(2, 0, 1)\n            full_tensor = torch.FloatTensor(full_norm).permute(2, 0, 1)\n            \n            return quarter_tensor, full_tensor\n            \n        except Exception as e:\n            print(f\"‚ùå Errore {idx}: {e}\")\n            return torch.zeros(3, self.input_size, self.input_size), \\\n                   torch.zeros(3, self.output_size, self.output_size)\n\n# Test\nprint(f\"\\nüß™ TEST DATASET:\")\n\nif 'ct_pairs' in globals() and len(ct_pairs) > 0:\n    dataset = CTCombinedDataset(ct_pairs[:100], augment=False)\n    \n    sample_input, sample_target = dataset[0]\n    \n    print(f\"\\n‚úÖ Sample caricato:\")\n    print(f\"   Input shape: {sample_input.shape}\")\n    print(f\"   Input range: [{sample_input.min():.3f}, {sample_input.max():.3f}]\")\n    print(f\"   Target shape: {sample_target.shape}\")  \n    print(f\"   Target range: [{sample_target.min():.3f}, {sample_target.max():.3f}]\")\n    \n    # Visualizza\n    import matplotlib.pyplot as plt\n    \n    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n    \n    # Denormalizza per visualizzazione (ora da [0,1] a [0,255])\n    input_vis = (sample_input.permute(1, 2, 0) * 255).numpy().astype(np.uint8)\n    target_vis = (sample_target.permute(1, 2, 0) * 255).numpy().astype(np.uint8)\n    \n    axes[0].imshow(input_vis)\n    axes[0].set_title(f'Input 96x96')\n    axes[0].axis('off')\n    \n    axes[1].imshow(target_vis)\n    axes[1].set_title(f'Target 384x384')\n    axes[1].axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n\nprint(f\"\\n‚úÖ DATASET LOADER PRONTO!\")\nprint(f\"üìä Normalizzazione: [0, 1] compatibile con Sigmoid\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T08:58:01.283168Z","iopub.execute_input":"2025-07-07T08:58:01.283889Z","iopub.status.idle":"2025-07-07T08:58:01.547010Z","shell.execute_reply.started":"2025-07-07T08:58:01.283859Z","shell.execute_reply":"2025-07-07T08:58:01.546230Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELLA 6 - Usa discriminator custom\nprint(\"üîß IMPORT E SETUP MODELLI - DISCRIMINATOR CUSTOM\")\nprint(\"=\"*60)\n\nimport sys\nimport os\nimport torch\nimport torch.nn as nn\n\n# Setup path e import Generator\nmedsrgan_path = './MedSRGAN'\nsys.path.insert(0, os.path.abspath(medsrgan_path))\nfrom generator import Generator\n\n# Discriminator semplificato che funziona per qualsiasi size\nclass SimpleDiscriminator(nn.Module):\n    \"\"\"Discriminator semplificato per 384x384\"\"\"\n    def __init__(self, in_channels=3):\n        super().__init__()\n        \n        def discriminator_block(in_feat, out_feat, normalize=True):\n            layers = [nn.Conv2d(in_feat, out_feat, 4, 2, 1)]\n            if normalize:\n                layers.append(nn.BatchNorm2d(out_feat))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n        \n        self.model = nn.Sequential(\n            # Concateniamo HR e LR all'inizio\n            # Input: 6 channels (3 HR + 3 LR upsampled)\n            *discriminator_block(in_channels * 2, 64, normalize=False),\n            *discriminator_block(64, 128),\n            *discriminator_block(128, 256),\n            *discriminator_block(256, 512),\n            *discriminator_block(512, 1024),\n            nn.AdaptiveAvgPool2d(1),\n            nn.Flatten(),\n            nn.Linear(1024, 1),\n            nn.Sigmoid()\n        )\n        \n        # Per upsampling LR a size di HR\n        self.upsample = nn.Upsample(scale_factor=4, mode='bilinear', align_corners=False)\n    \n    def forward(self, hr_img, lr_img):\n        # Upsample LR alla stessa dimensione di HR\n        lr_upsampled = self.upsample(lr_img)\n        # Concatena lungo i canali\n        combined = torch.cat([hr_img, lr_upsampled], dim=1)\n        return self.model(combined)\n\n# Setup modelli\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"\\nüñ•Ô∏è  Device: {device}\")\n\nprint(f\"\\nüèóÔ∏è  INIZIALIZZAZIONE MODELLI:\")\n\n# Generator originale\ngenerator = Generator(in_channels=3, blocks=8)\nprint(\"‚úÖ Generator MedSRGAN originale\")\n\n# Discriminator custom\ndiscriminator = SimpleDiscriminator(in_channels=3)\nprint(\"‚úÖ Discriminator custom per 384x384\")\n\n# To GPU\ngenerator = generator.to(device)\ndiscriminator = discriminator.to(device)\n\n# Test\nprint(f\"\\nüß™ TEST:\")\ntest_lr = torch.randn(2, 3, 96, 96).to(device)\ntest_hr = torch.randn(2, 3, 384, 384).to(device)\n\nwith torch.no_grad():\n    gen_out = generator(test_lr)\n    disc_out = discriminator(test_hr, test_lr)\n    print(f\"‚úÖ Generator: {test_lr.shape} ‚Üí {gen_out.shape}\")\n    print(f\"‚úÖ Discriminator: {disc_out.shape}\")\n\ngen_params = sum(p.numel() for p in generator.parameters())\ndisc_params = sum(p.numel() for p in discriminator.parameters())\n\nprint(f\"\\nüìä PARAMETRI:\")\nprint(f\"   Generator: {gen_params:,}\")\nprint(f\"   Discriminator: {disc_params:,}\")\n\nprint(f\"\\n‚úÖ SETUP COMPLETATO!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T08:58:14.039026Z","iopub.execute_input":"2025-07-07T08:58:14.039602Z","iopub.status.idle":"2025-07-07T08:58:14.373778Z","shell.execute_reply.started":"2025-07-07T08:58:14.039583Z","shell.execute_reply":"2025-07-07T08:58:14.373177Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELLA 7 AGGIORNATA: Loss Functions (fix deprecation warning)\nprint(\"üìä SETUP LOSS FUNCTIONS E FEATURE EXTRACTOR\")\nprint(\"=\"*60)\n\nimport torch\nimport torch.nn as nn\nimport torchvision.models as models\nfrom torchvision.models import VGG19_Weights\n\n# Feature Extractor aggiornato\nclass FeatureExtractor(nn.Module):\n    \"\"\"Feature extractor basato su VGG19 per perceptual loss\"\"\"\n    def __init__(self):\n        super(FeatureExtractor, self).__init__()\n        # Usa la nuova API di torchvision\n        vgg19 = models.vgg19(weights=VGG19_Weights.IMAGENET1K_V1)\n        self.feature_layers = nn.ModuleList([\n            nn.Sequential(*list(vgg19.features.children())[:5]),   # conv1_2\n            nn.Sequential(*list(vgg19.features.children())[:10]),  # conv2_2\n            nn.Sequential(*list(vgg19.features.children())[:19]),  # conv3_4\n            nn.Sequential(*list(vgg19.features.children())[:28]),  # conv4_4\n        ])\n        \n        for param in self.parameters():\n            param.requires_grad = False\n            \n    def forward(self, x):\n        features = []\n        for layer in self.feature_layers:\n            features.append(layer(x))\n        return features\n\nprint(\"‚úÖ Feature Extractor definito\")\n\n# Loss Functions semplificate per output [batch, 1]\nclass ReGAINEDLosses:\n    \"\"\"Loss functions per ReGAINED-CT con discriminator custom\"\"\"\n    \n    def __init__(self, device):\n        self.device = device\n        \n        # Loss di base\n        self.l1_loss = nn.L1Loss().to(device)\n        self.mse_loss = nn.MSELoss().to(device)\n        self.bce_loss = nn.BCELoss().to(device)  # BCE semplice per Sigmoid output\n        \n        # Feature extractor\n        self.feature_extractor = FeatureExtractor().to(device)\n        self.feature_extractor.eval()\n        \n        print(f\"‚úÖ Loss functions inizializzate su {device}\")\n    \n    def content_loss(self, pred, target):\n        \"\"\"L1 loss per contenuto pixel-wise\"\"\"\n        return self.l1_loss(pred, target)\n    \n    def perceptual_loss(self, pred, target):\n        \"\"\"Perceptual loss usando features VGG\"\"\"\n        pred_features = self.feature_extractor(pred)\n        target_features = self.feature_extractor(target)\n        \n        p_loss = 0\n        for pf, tf in zip(pred_features, target_features):\n            p_loss += self.l1_loss(pf, tf)\n        \n        return p_loss / len(pred_features)\n    \n    def adversarial_loss_generator(self, pred_fake):\n        \"\"\"Loss per generator - vuole che il discriminator creda che sia reale\"\"\"\n        valid = torch.ones_like(pred_fake).to(self.device)\n        return self.bce_loss(pred_fake, valid)\n    \n    def adversarial_loss_discriminator(self, pred_real, pred_fake):\n        \"\"\"Loss per discriminator - distinguere reale da fake\"\"\"\n        valid = torch.ones_like(pred_real).to(self.device)\n        fake = torch.zeros_like(pred_fake).to(self.device)\n        \n        real_loss = self.bce_loss(pred_real, valid)\n        fake_loss = self.bce_loss(pred_fake, fake)\n        \n        return (real_loss + fake_loss) / 2\n    \n    def relativistic_average_loss_g(self, pred_real, pred_fake):\n        \"\"\"RaGAN loss per generator\"\"\"\n        return self.bce_loss(pred_fake - pred_real.mean(0, keepdim=True), \n                           torch.ones_like(pred_fake).to(self.device))\n    \n    def relativistic_average_loss_d(self, pred_real, pred_fake):\n        \"\"\"RaGAN loss per discriminator\"\"\"\n        real_loss = self.bce_loss(pred_real - pred_fake.mean(0, keepdim=True), \n                                torch.ones_like(pred_real).to(self.device))\n        fake_loss = self.bce_loss(pred_fake - pred_real.mean(0, keepdim=True), \n                                torch.zeros_like(pred_fake).to(self.device))\n        return (real_loss + fake_loss) / 2\n\n# Inizializza losses\nlosses = ReGAINEDLosses(device)\n\n# Test\nprint(f\"\\nüß™ TEST LOSS FUNCTIONS:\")\ntry:\n    test_pred = torch.rand(2, 3, 384, 384).to(device)\n    test_target = torch.rand(2, 3, 384, 384).to(device)\n    test_disc_output = torch.rand(2, 1).to(device)  # Output [batch, 1]\n    \n    content_l = losses.content_loss(test_pred, test_target)\n    perceptual_l = losses.perceptual_loss(test_pred, test_target)\n    adv_g = losses.adversarial_loss_generator(test_disc_output)\n    adv_d = losses.adversarial_loss_discriminator(test_disc_output, test_disc_output)\n    \n    print(f\"‚úÖ Content loss: {content_l.item():.4f}\")\n    print(f\"‚úÖ Perceptual loss: {perceptual_l.item():.4f}\")\n    print(f\"‚úÖ Adversarial G: {adv_g.item():.4f}\")\n    print(f\"‚úÖ Adversarial D: {adv_d.item():.4f}\")\n    \nexcept Exception as e:\n    print(f\"‚ùå Errore: {e}\")\n\n# Pesi loss\nloss_weights = {\n    'content_weight': 0.1,\n    'perceptual_weight': 0.1,\n    'adversarial_weight': 1.0,\n}\n\nprint(f\"\\nüìã PESI LOSS:\")\nfor name, weight in loss_weights.items():\n    print(f\"   {name}: {weight}\")\n\nprint(f\"\\n‚úÖ LOSS FUNCTIONS PRONTE!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T09:00:26.019825Z","iopub.execute_input":"2025-07-07T09:00:26.020436Z","iopub.status.idle":"2025-07-07T09:00:27.975642Z","shell.execute_reply.started":"2025-07-07T09:00:26.020416Z","shell.execute_reply":"2025-07-07T09:00:27.974876Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELLA 8: Preparazione Training - Optimizer, Scheduler e Utils\nprint(\"üéØ PREPARAZIONE TRAINING\")\nprint(\"=\"*60)\n\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, random_split\nimport os\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom datetime import datetime\nfrom tqdm import tqdm\n\n# Configurazione training\nTRAIN_CONFIG = {\n    'batch_size': 4,  # Ridotto per GPU memory\n    'num_epochs': 50,\n    'learning_rate_g': 1e-4,\n    'learning_rate_d': 1e-4,\n    'weight_decay': 1e-5,\n    'beta1': 0.9,\n    'beta2': 0.999,\n    'checkpoint_interval': 5,\n    'sample_interval': 2,\n    'num_workers': 2,\n    'pin_memory': True,\n    'amp_enabled': True,  # Automatic Mixed Precision per velocizzare\n}\n\nprint(\"üìã CONFIGURAZIONE TRAINING:\")\nfor key, value in TRAIN_CONFIG.items():\n    print(f\"   {key}: {value}\")\n\n# Crea directory per risultati\nos.makedirs('checkpoints', exist_ok=True)\nos.makedirs('samples', exist_ok=True)\nos.makedirs('logs', exist_ok=True)\n\n# Prepara dataset e dataloader\nprint(f\"\\nüìä PREPARAZIONE DATASET:\")\n\n# Split dataset in train/validation\ntrain_size = int(0.9 * len(ct_pairs))\nval_size = len(ct_pairs) - train_size\n\ntrain_pairs = ct_pairs[:train_size]\nval_pairs = ct_pairs[train_size:]\n\nprint(f\"   Totale coppie: {len(ct_pairs)}\")\nprint(f\"   Training set: {train_size}\")\nprint(f\"   Validation set: {val_size}\")\n\n# Crea dataset\ntrain_dataset = CTCombinedDataset(train_pairs, augment=True)\nval_dataset = CTCombinedDataset(val_pairs, augment=False)\n\n# Crea dataloader\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=TRAIN_CONFIG['batch_size'],\n    shuffle=True,\n    num_workers=TRAIN_CONFIG['num_workers'],\n    pin_memory=TRAIN_CONFIG['pin_memory'],\n    drop_last=True\n)\n\nval_loader = DataLoader(\n    val_dataset,\n    batch_size=TRAIN_CONFIG['batch_size'],\n    shuffle=False,\n    num_workers=TRAIN_CONFIG['num_workers'],\n    pin_memory=TRAIN_CONFIG['pin_memory']\n)\n\nprint(f\"   Batch per epoca: {len(train_loader)}\")\n\n# Ottimizzatori\nprint(f\"\\nüîß SETUP OTTIMIZZATORI:\")\n\noptimizer_G = optim.Adam(\n    generator.parameters(),\n    lr=TRAIN_CONFIG['learning_rate_g'],\n    betas=(TRAIN_CONFIG['beta1'], TRAIN_CONFIG['beta2']),\n    weight_decay=TRAIN_CONFIG['weight_decay']\n)\n\noptimizer_D = optim.Adam(\n    discriminator.parameters(),\n    lr=TRAIN_CONFIG['learning_rate_d'],\n    betas=(TRAIN_CONFIG['beta1'], TRAIN_CONFIG['beta2']),\n    weight_decay=TRAIN_CONFIG['weight_decay']\n)\n\nprint(\"‚úÖ Optimizer Adam inizializzati\")\n\n# Learning rate scheduler\nscheduler_G = optim.lr_scheduler.StepLR(optimizer_G, step_size=10, gamma=0.5)\nscheduler_D = optim.lr_scheduler.StepLR(optimizer_D, step_size=10, gamma=0.5)\n\nprint(\"‚úÖ Learning rate scheduler inizializzati\")\n\n# Automatic Mixed Precision\nif TRAIN_CONFIG['amp_enabled'] and torch.cuda.is_available():\n    scaler = torch.cuda.amp.GradScaler()\n    print(\"‚úÖ AMP (Automatic Mixed Precision) abilitato\")\nelse:\n    scaler = None\n    print(\"‚ö†Ô∏è  AMP non disponibile\")\n\n# Metriche\nclass MetricsTracker:\n    \"\"\"Tracker per metriche di training\"\"\"\n    def __init__(self):\n        self.reset()\n    \n    def reset(self):\n        self.losses = {\n            'g_total': [],\n            'g_content': [],\n            'g_perceptual': [],\n            'g_adversarial': [],\n            'd_total': []\n        }\n        self.psnr = []\n        self.ssim = []\n    \n    def update(self, loss_dict, prefix=''):\n        for key, value in loss_dict.items():\n            full_key = f\"{prefix}_{key}\" if prefix else key\n            if full_key in self.losses:\n                self.losses[full_key].append(value)\n    \n    def get_avg(self, key):\n        if key in self.losses and len(self.losses[key]) > 0:\n            return np.mean(self.losses[key])\n        return 0\n    \n    def get_all_avg(self):\n        return {key: self.get_avg(key) for key in self.losses.keys()}\n\nmetrics_tracker = MetricsTracker()\n\n# Funzioni utility\ndef save_checkpoint(epoch, generator, discriminator, optimizer_G, optimizer_D, metrics, prefix=''):\n    \"\"\"Salva checkpoint del modello\"\"\"\n    checkpoint = {\n        'epoch': epoch,\n        'generator_state_dict': generator.state_dict(),\n        'discriminator_state_dict': discriminator.state_dict(),\n        'optimizer_G_state_dict': optimizer_G.state_dict(),\n        'optimizer_D_state_dict': optimizer_D.state_dict(),\n        'metrics': metrics\n    }\n    \n    filename = f\"checkpoint_{prefix}_epoch_{epoch}.pth\"\n    filepath = os.path.join('checkpoints', filename)\n    torch.save(checkpoint, filepath)\n    print(f\"üíæ Checkpoint salvato: {filename}\")\n\ndef save_samples(epoch, generator, val_loader, num_samples=4):\n    \"\"\"Salva immagini di esempio\"\"\"\n    generator.eval()\n    \n    with torch.no_grad():\n        # Prendi un batch di validazione\n        val_batch = next(iter(val_loader))\n        low_res, high_res = val_batch\n        low_res = low_res[:num_samples].to(device)\n        high_res = high_res[:num_samples].to(device)\n        \n        # Genera immagini\n        fake_high_res = generator(low_res)\n        \n        # Denormalizza per visualizzazione [0, 1] ‚Üí [0, 255]\n        low_res = low_res * 255.0\n        fake_high_res = fake_high_res * 255.0\n        high_res = high_res * 255.0\n        \n        # Crea griglia\n        fig, axes = plt.subplots(num_samples, 3, figsize=(12, 4*num_samples))\n        \n        for i in range(num_samples):\n            # Input low-res\n            axes[i, 0].imshow(low_res[i].cpu().permute(1, 2, 0).numpy().astype(np.uint8))\n            axes[i, 0].set_title('Input (96x96)')\n            axes[i, 0].axis('off')\n            \n            # Generated\n            axes[i, 1].imshow(fake_high_res[i].cpu().permute(1, 2, 0).numpy().astype(np.uint8))\n            axes[i, 1].set_title('Generated (384x384)')\n            axes[i, 1].axis('off')\n            \n            # Target\n            axes[i, 2].imshow(high_res[i].cpu().permute(1, 2, 0).numpy().astype(np.uint8))\n            axes[i, 2].set_title('Target (384x384)')\n            axes[i, 2].axis('off')\n        \n        plt.tight_layout()\n        plt.savefig(f'samples/epoch_{epoch:03d}.png', dpi=150, bbox_inches='tight')\n        plt.close()\n        \n    generator.train()\n\ndef calculate_psnr(img1, img2):\n    \"\"\"Calcola PSNR tra due immagini normalizzate [0, 1]\"\"\"\n    mse = torch.mean((img1 - img2) ** 2)\n    if mse == 0:\n        return float('inf')\n    # PSNR con max value = 1.0 per normalizzazione [0, 1]\n    return 20 * torch.log10(1.0 / torch.sqrt(mse))\n\nprint(f\"\\n‚úÖ PREPARAZIONE TRAINING COMPLETATA!\")\nprint(f\"üéØ Pronto per iniziare il training con:\")\nprint(f\"   ‚Ä¢ {len(train_loader)} batch per epoca\")\nprint(f\"   ‚Ä¢ {TRAIN_CONFIG['num_epochs']} epoche totali\")\nprint(f\"   ‚Ä¢ Checkpoint ogni {TRAIN_CONFIG['checkpoint_interval']} epoche\")\nprint(f\"   ‚Ä¢ Samples ogni {TRAIN_CONFIG['sample_interval']} epoche\")\nprint(f\"   ‚Ä¢ Normalizzazione: [0, 1]\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T08:58:34.158552Z","iopub.execute_input":"2025-07-07T08:58:34.158864Z","iopub.status.idle":"2025-07-07T08:58:34.188215Z","shell.execute_reply.started":"2025-07-07T08:58:34.158816Z","shell.execute_reply":"2025-07-07T08:58:34.187493Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELLA 9 AGGIORNATA: Training Loop ReGAINED-CT (Fix deprecation warning)\nprint(\"üöÄ TRAINING LOOP ReGAINED-CT\")\nprint(\"=\"*60)\n\nimport time\nimport gc\nimport warnings\n\n# Funzione di training per un'epoca\ndef train_epoch(epoch, generator, discriminator, train_loader, optimizer_G, optimizer_D, \n                losses, loss_weights, metrics_tracker, scaler=None):\n    \"\"\"Training per una singola epoca\"\"\"\n    \n    generator.train()\n    discriminator.train()\n    \n    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch}\")\n    \n    for batch_idx, (low_res, high_res) in enumerate(progress_bar):\n        low_res = low_res.to(device)\n        high_res = high_res.to(device)\n        \n        # ---------------------\n        # Train Discriminator\n        # ---------------------\n        optimizer_D.zero_grad()\n        \n        if scaler and TRAIN_CONFIG['amp_enabled']:\n            # FIX: Usa la nuova sintassi per autocast\n            with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n                # Generate fake images\n                fake_high_res = generator(low_res)\n                \n                # Get discriminator predictions\n                pred_real = discriminator(high_res, low_res)\n                pred_fake = discriminator(fake_high_res.detach(), low_res)\n                \n                # Calculate discriminator loss\n                d_loss = losses.relativistic_average_loss_d(pred_real, pred_fake)\n            \n            scaler.scale(d_loss).backward()\n            scaler.step(optimizer_D)\n            scaler.update()\n        else:\n            # Without AMP\n            fake_high_res = generator(low_res)\n            pred_real = discriminator(high_res, low_res)\n            pred_fake = discriminator(fake_high_res.detach(), low_res)\n            d_loss = losses.relativistic_average_loss_d(pred_real, pred_fake)\n            d_loss.backward()\n            optimizer_D.step()\n        \n        # ---------------------\n        # Train Generator\n        # ---------------------\n        optimizer_G.zero_grad()\n        \n        if scaler and TRAIN_CONFIG['amp_enabled']:\n            # FIX: Usa la nuova sintassi per autocast\n            with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n                # Generate fake images\n                fake_high_res = generator(low_res)\n                \n                # Content loss\n                content_loss = losses.content_loss(fake_high_res, high_res)\n                \n                # Perceptual loss\n                perceptual_loss = losses.perceptual_loss(fake_high_res, high_res)\n                \n                # Adversarial loss\n                pred_fake = discriminator(fake_high_res, low_res)\n                pred_real = discriminator(high_res, low_res)\n                adversarial_loss = losses.relativistic_average_loss_g(pred_real, pred_fake)\n                \n                # Total generator loss\n                g_loss = (content_loss * loss_weights['content_weight'] + \n                         perceptual_loss * loss_weights['perceptual_weight'] + \n                         adversarial_loss * loss_weights['adversarial_weight'])\n            \n            scaler.scale(g_loss).backward()\n            scaler.step(optimizer_G)\n            scaler.update()\n        else:\n            # Without AMP\n            fake_high_res = generator(low_res)\n            content_loss = losses.content_loss(fake_high_res, high_res)\n            perceptual_loss = losses.perceptual_loss(fake_high_res, high_res)\n            pred_fake = discriminator(fake_high_res, low_res)\n            pred_real = discriminator(high_res, low_res)\n            adversarial_loss = losses.relativistic_average_loss_g(pred_real, pred_fake)\n            g_loss = (content_loss * loss_weights['content_weight'] + \n                     perceptual_loss * loss_weights['perceptual_weight'] + \n                     adversarial_loss * loss_weights['adversarial_weight'])\n            g_loss.backward()\n            optimizer_G.step()\n        \n        # Update metrics\n        metrics_tracker.update({\n            'g_total': g_loss.item(),\n            'g_content': content_loss.item() * loss_weights['content_weight'],\n            'g_perceptual': perceptual_loss.item() * loss_weights['perceptual_weight'],\n            'g_adversarial': adversarial_loss.item() * loss_weights['adversarial_weight'],\n            'd_total': d_loss.item()\n        })\n        \n        # Update progress bar\n        progress_bar.set_postfix({\n            'G_loss': f\"{g_loss.item():.4f}\",\n            'D_loss': f\"{d_loss.item():.4f}\"\n        })\n        \n        # Clean memory periodically\n        if batch_idx % 100 == 0:\n            torch.cuda.empty_cache()\n            gc.collect()\n    \n    return metrics_tracker.get_all_avg()\n\n# Funzione di validazione\ndef validate(generator, val_loader, losses):\n    \"\"\"Validazione del modello\"\"\"\n    generator.eval()\n    val_losses = []\n    val_psnr = []\n    \n    with torch.no_grad():\n        for low_res, high_res in tqdm(val_loader, desc=\"Validation\", leave=False):\n            low_res = low_res.to(device)\n            high_res = high_res.to(device)\n            \n            # Generate fake images\n            fake_high_res = generator(low_res)\n            \n            # Calculate validation loss\n            val_loss = losses.content_loss(fake_high_res, high_res)\n            val_losses.append(val_loss.item())\n            \n            # Calculate PSNR\n            psnr = calculate_psnr(fake_high_res, high_res)\n            val_psnr.append(psnr.item())\n    \n    return np.mean(val_losses), np.mean(val_psnr)\n\n# Training loop principale\nprint(f\"\\nüèãÔ∏è INIZIO TRAINING\")\nprint(f\"   Device: {device}\")\nprint(f\"   Batch: {TRAIN_CONFIG['batch_size']}\")\nprint(f\"   Epoche: {TRAIN_CONFIG['num_epochs']}\")\n\nbest_val_loss = float('inf')\nbest_val_psnr = 0\ntraining_history = {\n    'g_loss': [], 'd_loss': [], 'val_loss': [], 'val_psnr': []\n}\n\nstart_time = time.time()\n\ntry:\n    for epoch in range(1, TRAIN_CONFIG['num_epochs'] + 1):\n        print(f\"\\n{'='*60}\")\n        print(f\"EPOCA {epoch}/{TRAIN_CONFIG['num_epochs']}\")\n        print(f\"{'='*60}\")\n        \n        # Reset metrics\n        metrics_tracker.reset()\n        \n        # Train for one epoch\n        epoch_metrics = train_epoch(\n            epoch, generator, discriminator, train_loader,\n            optimizer_G, optimizer_D, losses, loss_weights,\n            metrics_tracker, scaler\n        )\n        \n        # Print epoch metrics\n        print(f\"\\nüìä Metriche Epoca {epoch}:\")\n        print(f\"   G Total: {epoch_metrics['g_total']:.4f}\")\n        print(f\"   G Content: {epoch_metrics['g_content']:.4f}\")\n        print(f\"   G Perceptual: {epoch_metrics['g_perceptual']:.4f}\")\n        print(f\"   G Adversarial: {epoch_metrics['g_adversarial']:.4f}\")\n        print(f\"   D Total: {epoch_metrics['d_total']:.4f}\")\n        \n        # Validation\n        if epoch % 2 == 0:\n            val_loss, val_psnr = validate(generator, val_loader, losses)\n            print(f\"\\nüìà Validazione:\")\n            print(f\"   Val Loss: {val_loss:.4f}\")\n            print(f\"   Val PSNR: {val_psnr:.2f} dB\")\n            \n            training_history['val_loss'].append(val_loss)\n            training_history['val_psnr'].append(val_psnr)\n            \n            # Save best model\n            if val_psnr > best_val_psnr:\n                best_val_psnr = val_psnr\n                best_val_loss = val_loss\n                save_checkpoint(epoch, generator, discriminator, \n                              optimizer_G, optimizer_D, \n                              {'val_loss': val_loss, 'val_psnr': val_psnr}, \n                              'best')\n                print(f\"   üèÜ Nuovo miglior modello! PSNR: {val_psnr:.2f} dB\")\n        \n        # Update learning rate\n        scheduler_G.step()\n        scheduler_D.step()\n        \n        # Save checkpoint\n        if epoch % TRAIN_CONFIG['checkpoint_interval'] == 0:\n            save_checkpoint(epoch, generator, discriminator, \n                          optimizer_G, optimizer_D, epoch_metrics, 'regular')\n        \n        # Save samples\n        if epoch % TRAIN_CONFIG['sample_interval'] == 0:\n            save_samples(epoch, generator, val_loader)\n            print(f\"   üé® Samples salvati\")\n        \n        # Store history\n        training_history['g_loss'].append(epoch_metrics['g_total'])\n        training_history['d_loss'].append(epoch_metrics['d_total'])\n        \n        # Time estimate\n        elapsed = time.time() - start_time\n        eta = (elapsed / epoch) * (TRAIN_CONFIG['num_epochs'] - epoch)\n        print(f\"\\n‚è±Ô∏è  Tempo: {elapsed/60:.1f} min | ETA: {eta/60:.1f} min\")\n\nexcept KeyboardInterrupt:\n    print(f\"\\n‚ö†Ô∏è  Training interrotto!\")\n    save_checkpoint(epoch, generator, discriminator, \n                   optimizer_G, optimizer_D, metrics_tracker.get_all_avg(), 'interrupted')\n\nexcept Exception as e:\n    print(f\"\\n‚ùå Errore durante training: {e}\")\n    import traceback\n    traceback.print_exc()\n\nfinally:\n    # Plot training history\n    if len(training_history['g_loss']) > 0:\n        plt.figure(figsize=(15, 5))\n        \n        plt.subplot(1, 3, 1)\n        plt.plot(training_history['g_loss'], label='G Loss', linewidth=2)\n        plt.plot(training_history['d_loss'], label='D Loss', linewidth=2)\n        plt.xlabel('Epoch')\n        plt.ylabel('Loss')\n        plt.title('Training Losses')\n        plt.legend()\n        plt.grid(True, alpha=0.3)\n        \n        if len(training_history['val_loss']) > 0:\n            plt.subplot(1, 3, 2)\n            plt.plot(training_history['val_loss'], 'g-', linewidth=2)\n            plt.xlabel('Epoch')\n            plt.ylabel('Validation Loss')\n            plt.title('Validation Loss')\n            plt.grid(True, alpha=0.3)\n            \n            plt.subplot(1, 3, 3)\n            plt.plot(training_history['val_psnr'], 'b-', linewidth=2)\n            plt.xlabel('Epoch')\n            plt.ylabel('PSNR (dB)')\n            plt.title('Validation PSNR')\n            plt.grid(True, alpha=0.3)\n        \n        plt.tight_layout()\n        plt.savefig('training_history.png', dpi=300, bbox_inches='tight')\n        plt.show()\n\nprint(f\"\\n‚úÖ TRAINING COMPLETATO!\")\nprint(f\"‚è±Ô∏è  Tempo totale: {(time.time() - start_time)/60:.1f} minuti\")\nprint(f\"üèÜ Miglior PSNR: {best_val_psnr:.2f} dB\")\nprint(f\"üìâ Miglior val loss: {best_val_loss:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T09:07:48.763655Z","iopub.execute_input":"2025-07-07T09:07:48.763970Z","iopub.status.idle":"2025-07-07T09:07:48.881986Z","shell.execute_reply.started":"2025-07-07T09:07:48.763946Z","shell.execute_reply":"2025-07-07T09:07:48.880150Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true},"outputs":[{"name":"stdout","text":"üöÄ TRAINING LOOP ReGAINED-CT\n============================================================\n\nüèãÔ∏è INIZIO TRAINING\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/584040554.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;31m# Training loop principale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nüèãÔ∏è INIZIO TRAINING\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"   Device: {device}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"   Batch: {TRAIN_CONFIG['batch_size']}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"   Epoche: {TRAIN_CONFIG['num_epochs']}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'device' is not defined"],"ename":"NameError","evalue":"name 'device' is not defined","output_type":"error"}],"execution_count":1},{"cell_type":"code","source":"# CELLA 10: Evaluation del Modello Trainato\nprint(\"üî¨ EVALUATION DEL MODELLO TRAINATO\")\nprint(\"=\"*60)\n\nimport glob\nfrom skimage.metrics import structural_similarity as ssim\nimport numpy as np\n\ndef load_best_model():\n    \"\"\"Carica il miglior modello salvato\"\"\"\n    checkpoint_patterns = [\n        'checkpoints/checkpoint_best_*.pth',\n        'checkpoints/checkpoint_regular_*.pth',\n        'checkpoints/checkpoint_interrupted_*.pth'\n    ]\n    \n    all_checkpoints = []\n    for pattern in checkpoint_patterns:\n        all_checkpoints.extend(glob.glob(pattern))\n    \n    if not all_checkpoints:\n        print(\"‚ùå Nessun checkpoint trovato\")\n        return None\n    \n    # Trova il pi√π recente\n    latest_checkpoint = max(all_checkpoints, key=os.path.getctime)\n    print(f\"üìã Caricamento checkpoint: {os.path.basename(latest_checkpoint)}\")\n    \n    checkpoint = torch.load(latest_checkpoint, map_location=device)\n    \n    # Ricrea il generator\n    loaded_generator = Generator(in_channels=3, blocks=8).to(device)\n    loaded_generator.load_state_dict(checkpoint['generator_state_dict'])\n    loaded_generator.eval()\n    \n    epoch = checkpoint.get('epoch', 'Unknown')\n    print(f\"‚úÖ Modello caricato - Epoca: {epoch}\")\n    \n    return loaded_generator\n\ndef calculate_metrics(generated, target):\n    \"\"\"Calcola metriche di qualit√†\"\"\"\n    # Converti in numpy e denormalizza\n    gen_np = ((generated + 1.0) * 127.5).clamp(0, 255).cpu().numpy().astype(np.uint8)\n    tar_np = ((target + 1.0) * 127.5).clamp(0, 255).cpu().numpy().astype(np.uint8)\n    \n    # PSNR\n    mse = np.mean((gen_np.astype(np.float32) - tar_np.astype(np.float32)) ** 2)\n    psnr = 20 * np.log10(255.0 / np.sqrt(mse)) if mse > 0 else float('inf')\n    \n    # SSIM (sul primo canale per immagini RGB)\n    ssim_value = ssim(gen_np[0, 0], tar_np[0, 0], data_range=255)\n    \n    return {\n        'psnr': psnr,\n        'ssim': ssim_value,\n        'mse': mse\n    }\n\ndef evaluate_on_test_set(generator, test_loader, num_samples=20):\n    \"\"\"Valutazione completa su test set\"\"\"\n    generator.eval()\n    \n    all_metrics = {\n        'psnr': [],\n        'ssim': [],\n        'mse': []\n    }\n    \n    print(f\"\\nüìä Valutazione su {num_samples} campioni...\")\n    \n    with torch.no_grad():\n        for i, (low_res, high_res) in enumerate(tqdm(test_loader, desc=\"Evaluation\")):\n            if i >= num_samples:\n                break\n            \n            low_res = low_res.to(device)\n            high_res = high_res.to(device)\n            \n            # Genera immagine\n            fake_high_res = generator(low_res)\n            \n            # Calcola metriche\n            metrics = calculate_metrics(fake_high_res, high_res)\n            \n            for key, value in metrics.items():\n                all_metrics[key].append(value)\n            \n            # Salva qualche esempio\n            if i < 5:\n                save_evaluation_sample(low_res[0], fake_high_res[0], high_res[0], \n                                     f'evaluation_sample_{i}.png', metrics)\n    \n    # Statistiche finali\n    print(f\"\\nüìà RISULTATI EVALUATION:\")\n    print(f\"   PSNR: {np.mean(all_metrics['psnr']):.2f} ¬± {np.std(all_metrics['psnr']):.2f} dB\")\n    print(f\"   SSIM: {np.mean(all_metrics['ssim']):.4f} ¬± {np.std(all_metrics['ssim']):.4f}\")\n    print(f\"   MSE: {np.mean(all_metrics['mse']):.2f} ¬± {np.std(all_metrics['mse']):.2f}\")\n    \n    return all_metrics\n\ndef save_evaluation_sample(low_res, generated, target, filename, metrics):\n    \"\"\"Salva esempio con metriche\"\"\"\n    # Denormalizza\n    low_res = (low_res + 1) * 127.5\n    generated = (generated + 1) * 127.5\n    target = (target + 1) * 127.5\n    \n    fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n    \n    # Input\n    axes[0].imshow(low_res.cpu().permute(1, 2, 0).numpy().astype(np.uint8))\n    axes[0].set_title('Input 96x96\\n(Quarter Dose Downscaled)')\n    axes[0].axis('off')\n    \n    # Generated\n    axes[1].imshow(generated.cpu().permute(1, 2, 0).numpy().astype(np.uint8))\n    axes[1].set_title(f'Generated 384x384\\nPSNR: {metrics[\"psnr\"]:.2f} dB')\n    axes[1].axis('off')\n    \n    # Target\n    axes[2].imshow(target.cpu().permute(1, 2, 0).numpy().astype(np.uint8))\n    axes[2].set_title('Target 384x384\\n(Full Dose)')\n    axes[2].axis('off')\n    \n    # Difference map\n    diff = torch.abs(generated - target).mean(dim=0)\n    im = axes[3].imshow(diff.cpu().numpy(), cmap='hot')\n    axes[3].set_title(f'Difference Map\\nSSIM: {metrics[\"ssim\"]:.3f}')\n    axes[3].axis('off')\n    plt.colorbar(im, ax=axes[3], fraction=0.046)\n    \n    plt.tight_layout()\n    os.makedirs('evaluation', exist_ok=True)\n    plt.savefig(f'evaluation/{filename}', dpi=150, bbox_inches='tight')\n    plt.close()\n\n# Esegui evaluation\nprint(\"\\nüöÄ AVVIO EVALUATION...\")\n\n# Carica modello\nbest_generator = load_best_model()\n\nif best_generator:\n    # Crea test loader\n    test_dataset = CTCombinedDataset(val_pairs[:50], augment=False)  # Usa subset per test\n    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n    \n    # Valuta\n    results = evaluate_on_test_set(best_generator, test_loader, num_samples=20)\n    \n    # Plot distribuzione metriche\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n    \n    ax1.hist(results['psnr'], bins=20, alpha=0.7, color='blue')\n    ax1.axvline(np.mean(results['psnr']), color='red', linestyle='--', label=f'Mean: {np.mean(results[\"psnr\"]):.2f}')\n    ax1.set_xlabel('PSNR (dB)')\n    ax1.set_ylabel('Count')\n    ax1.set_title('PSNR Distribution')\n    ax1.legend()\n    ax1.grid(True, alpha=0.3)\n    \n    ax2.hist(results['ssim'], bins=20, alpha=0.7, color='green')\n    ax2.axvline(np.mean(results['ssim']), color='red', linestyle='--', label=f'Mean: {np.mean(results[\"ssim\"]):.3f}')\n    ax2.set_xlabel('SSIM')\n    ax2.set_ylabel('Count')\n    ax2.set_title('SSIM Distribution')\n    ax2.legend()\n    ax2.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.savefig('evaluation/metrics_distribution.png', dpi=150)\n    plt.show()\nelse:\n    print(\"‚ùå Impossibile caricare il modello per evaluation\")","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELLA 11: Test su DICOM Reali del Dataset\nprint(\"üè• TEST SU DICOM REALI\")\nprint(\"=\"*60)\n\nimport pydicom\nimport cv2\n\nclass DicomInference:\n    \"\"\"Inference su file DICOM reali\"\"\"\n    \n    def __init__(self, generator, device):\n        self.generator = generator\n        self.device = device\n        self.generator.eval()\n    \n    def load_dicom(self, dicom_path, window_center=50, window_width=350):\n        \"\"\"Carica e preprocessa DICOM\"\"\"\n        # Leggi DICOM\n        dcm = pydicom.dcmread(dicom_path)\n        img_array = dcm.pixel_array.astype(np.float32)\n        \n        # Applica rescale slope/intercept\n        if hasattr(dcm, 'RescaleSlope') and hasattr(dcm, 'RescaleIntercept'):\n            img_array = img_array * dcm.RescaleSlope + dcm.RescaleIntercept\n        \n        # Window/Level per tessuti molli\n        img_min = window_center - window_width // 2\n        img_max = window_center + window_width // 2\n        img_windowed = np.clip(img_array, img_min, img_max)\n        \n        # Normalizza a 0-255\n        img_normalized = ((img_windowed - img_min) / (img_max - img_min) * 255).astype(np.uint8)\n        \n        return img_normalized, dcm\n    \n    def preprocess_for_model(self, img_array, input_size=96):\n        \"\"\"Prepara immagine per il modello\"\"\"\n        # Resize a input size\n        img_resized = cv2.resize(img_array, (input_size, input_size), \n                                interpolation=cv2.INTER_CUBIC)\n        \n        # Converti in RGB (replica canale)\n        img_rgb = np.stack([img_resized] * 3, axis=-1)\n        \n        # Normalizza a [-1, 1]\n        img_norm = (img_rgb / 127.5) - 1.0\n        \n        # To tensor\n        img_tensor = torch.FloatTensor(img_norm).permute(2, 0, 1).unsqueeze(0)\n        \n        return img_tensor.to(self.device)\n    \n    def postprocess_output(self, output_tensor, target_size):\n        \"\"\"Post-processa output del modello\"\"\"\n        # Denormalizza\n        output = (output_tensor + 1) * 127.5\n        output = output.clamp(0, 255)\n        \n        # Prendi primo canale (grayscale)\n        output_gray = output[0, 0].cpu().numpy().astype(np.uint8)\n        \n        # Resize a dimensione target se necessario\n        if output_gray.shape != target_size:\n            output_resized = cv2.resize(output_gray, target_size, \n                                      interpolation=cv2.INTER_CUBIC)\n            return output_resized\n        \n        return output_gray\n    \n    def process_dicom_pair(self, quarter_path, full_path, save_results=True):\n        \"\"\"Processa coppia quarter/full dose DICOM\"\"\"\n        print(f\"\\nüìÑ Processing DICOM pair:\")\n        print(f\"   Quarter: {os.path.basename(quarter_path)}\")\n        print(f\"   Full: {os.path.basename(full_path)}\")\n        \n        # Carica DICOM\n        quarter_img, quarter_dcm = self.load_dicom(quarter_path)\n        full_img, full_dcm = self.load_dicom(full_path)\n        \n        print(f\"   Original size: {quarter_img.shape}\")\n        \n        # Preprocess per modello\n        input_tensor = self.preprocess_for_model(quarter_img)\n        \n        # Inference\n        with torch.no_grad():\n            output_tensor = self.generator(input_tensor)\n        \n        # Postprocess\n        enhanced_img = self.postprocess_output(output_tensor, quarter_img.shape)\n        \n        # Calcola metriche\n        # Resize full dose per confronto equo\n        full_resized = cv2.resize(full_img, enhanced_img.shape[::-1], \n                                interpolation=cv2.INTER_CUBIC)\n        \n        mse = np.mean((enhanced_img.astype(np.float32) - full_resized.astype(np.float32)) ** 2)\n        psnr = 20 * np.log10(255.0 / np.sqrt(mse)) if mse > 0 else float('inf')\n        \n        # SSIM\n        ssim_value = ssim(enhanced_img, full_resized, data_range=255)\n        \n        print(f\"   üìä Risultati:\")\n        print(f\"      PSNR: {psnr:.2f} dB\")\n        print(f\"      SSIM: {ssim_value:.4f}\")\n        \n        if save_results:\n            self.save_dicom_results(quarter_img, enhanced_img, full_img, \n                                  psnr, ssim_value, quarter_path)\n        \n        return enhanced_img, {'psnr': psnr, 'ssim': ssim_value}\n    \n    def save_dicom_results(self, original, enhanced, target, psnr, ssim_val, original_path):\n        \"\"\"Salva risultati confronto DICOM\"\"\"\n        fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n        \n        # Prima riga: immagini complete\n        axes[0, 0].imshow(original, cmap='gray')\n        axes[0, 0].set_title(f'Quarter Dose DICOM\\n{original.shape}')\n        axes[0, 0].axis('off')\n        \n        axes[0, 1].imshow(enhanced, cmap='gray')\n        axes[0, 1].set_title(f'Enhanced (Model Output)\\nPSNR: {psnr:.2f} dB')\n        axes[0, 1].axis('off')\n        \n        axes[0, 2].imshow(target, cmap='gray')\n        axes[0, 2].set_title(f'Full Dose DICOM\\nSSIM: {ssim_val:.4f}')\n        axes[0, 2].axis('off')\n        \n        # Seconda riga: zoom su ROI centrale\n        h, w = original.shape\n        roi_size = min(h, w) // 3\n        center_h, center_w = h // 2, w // 2\n        \n        roi_slice = (slice(center_h - roi_size//2, center_h + roi_size//2),\n                    slice(center_w - roi_size//2, center_w + roi_size//2))\n        \n        axes[1, 0].imshow(original[roi_slice], cmap='gray')\n        axes[1, 0].set_title('Quarter Dose (Zoom)')\n        axes[1, 0].axis('off')\n        \n        axes[1, 1].imshow(enhanced[roi_slice], cmap='gray')\n        axes[1, 1].set_title('Enhanced (Zoom)')\n        axes[1, 1].axis('off')\n        \n        axes[1, 2].imshow(target[roi_slice], cmap='gray')\n        axes[1, 2].set_title('Full Dose (Zoom)')\n        axes[1, 2].axis('off')\n        \n        plt.tight_layout()\n        \n        # Salva\n        filename = f'dicom_result_{os.path.basename(original_path).replace(\".IMA\", \".png\")}'\n        os.makedirs('dicom_results', exist_ok=True)\n        plt.savefig(f'dicom_results/{filename}', dpi=150, bbox_inches='tight')\n        plt.close()\n\n# Test su DICOM reali\nif best_generator:\n    print(\"\\nüîç Ricerca file DICOM nel dataset...\")\n    \n    # Trova alcuni DICOM di test\n    dataset_path = \"/kaggle/input/ct-low-dose-reconstruction/\"\n    dicom_files = glob.glob(os.path.join(dataset_path, \"**/*.IMA\"), recursive=True)\n    \n    quarter_dicoms = [f for f in dicom_files if 'quarter' in f.lower() or 'qd' in f.lower()]\n    full_dicoms = [f for f in dicom_files if 'full' in f.lower() or 'fd' in f.lower()]\n    \n    print(f\"   Trovati: {len(quarter_dicoms)} quarter dose, {len(full_dicoms)} full dose\")\n    \n    if quarter_dicoms and full_dicoms:\n        # Crea oggetto inference\n        dicom_inference = DicomInference(best_generator, device)\n        \n        # Test su prime 3 coppie\n        print(f\"\\nüß™ Test su 3 coppie DICOM...\")\n        \n        for i in range(min(3, len(quarter_dicoms), len(full_dicoms))):\n            # Per semplicit√†, usa i primi file (in produzione dovresti fare matching corretto)\n            dicom_inference.process_dicom_pair(\n                quarter_dicoms[i], \n                full_dicoms[i], \n                save_results=True\n            )\n    else:\n        print(\"‚ö†Ô∏è  Nessun DICOM trovato, uso PNG per test...\")\n        # Fallback su PNG se non ci sono DICOM","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null}]}